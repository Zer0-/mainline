- Trying 10,000 nodes on one port resulted in running out of memory (less than 8Gb were available)
- About 330,000 nodes were added to the routing tables, my very rough estimate is that nodes add between
  100 and 200 peers to their RT, so optimistically it's possible we were a third of the way there
- This is before any kind of timed cleanup is implemented

- trying 1000 nodes after stm upgrade (also with a fix for the main memory issue
    but still on a single port) peaked cpu at 286% (avg about 120-150%)
    and 38% memory, 397k+ nodes added, 37k unique info hashes announced, more are being added.
    There is no strict end condition to this test.

    - I hope the cpu usage goes down after the warmup is complete, there is a chance
    that we are just going through the warmup process as fast as we can on one port
    (and therefore one read thread).

    - pausing xterm scrolling leaves cpu at 8%!




Plan
====

- Host nodes on multiple ports
- Resolve info_hashes and start collecting data
    - ask the database if it has the info_hash first
        - connect to pg
        - init pg table(s)
        - query and write rows
            - full text search column
    - initiate GetPeers for info hash
    - propagate subs and cmds from ResolveMagnet
        - probably should add support for multiple peers there


DB stuff
--------

- Where to keep open connection pool?
    - writeThreadS entry of (TQueue DBSession, TVar Bool, ThreadId)
        - ignore quit flag?

    - write thread needs persistent state?

- Cmd interface?
    - Cmd.db :: Session -> Cmd msg

- DB metadata table
    - info_hash (PK)
    - name
    - time added
    - file list?
    - File table ?
        - name
        - path
        - size
        - part of metadata
    - search index

Maybe
-----

Move Mainline to Network.Mainline?
Mainline/Scripts/SendUDPHello.hs to a common Runnables directory with App?
Then we have Network.{Mainline, KRPC, Bittorrent}
- but we still have all of the actual network calls under Architecture


Priority
--------

- What is the database interface for our TEA style framework?
    - We need a connection string from the lib user
        - Should this be passed to run via Config? Or should this be passed
            - ConfigWithDB model msg schemas, runwdb
            - rename Config to Program?
            - Cons of this are that we cannot issue Cmds to get the connection string dynamically
            - There is only one allowed db pool however, so only one database can be used at a time.
                Given that we've accepted this limitation maybe it's ok to pass
                the connection string at the beggining.
        - on a per-command basis?
            - Cons of this are passing the same string each time we run a cmd
    - Need a query
    - Need a session (which probably tells squeal how to get the result)

- Actually we first just need to query our db for an infohash to see if we should get
  the metainfo for it. How is that so hard?

Misc
----
  - in a lib we depend on, BEncode instance for InfoDict encodes the dict it
  is given to get the info hash. This is wasteful since the dict itself was probably
  just decoded from a bytestring. Maybe the info hash need not be part of the
  data structure so we can just compute it from the original bytestring?
